# 深度学习要点梳理和个人理解

这里记录下对深度学习某些知识和内容的探究、梳理，以及个人理解。



## 写在前面

:notes: 有一些学习要点的笔记和梳理我有随记在「幕布」和「有道云笔记」平台，挺杂乱的，但值得看一看：

- [01-机器学习&深度学习要点小梳理（含资料推荐）](https://mubu.com/doc/2E8oghDU78)
- [02-卷积神经网络CNN探究(含反卷积、转置卷积、空洞卷积、上采样、下采样)](https://mubu.com/doc/3LbSzN4z-8)
- [03-CNN网络模型演进(LetNet、AleNet、VGGNet、NIN、GoogleNet、ResNet等)](https://mubu.com/doc/2BFlc9r-B8)
- [04-图像语义分割(含FCN、UNet、SegNet、PSPNet、Deeplabv1&v2&v3等)](https://mubu.com/doc/1OEfnuDXAc)
- [数据预处理：中心化（又叫零均值化）和标准化（又叫归一化）](https://mubu.com/doc/2qC5u7sGw8)
- [对softmax，softmax loss和cross entropy的理解](https://mubu.com/doc/T0NtYmnFc)
- ……

里面的内容有的会继续补充，另外，有新作的要点和内容，我会以分享链接的形式继续更新在上面，不过以上的分享链接也有可能指不定哪天被我取消了，不过关注该仓库即可，日后有时间整理成文至此。

注：对于在「幕布」上作的内容，有的我有导出成了 HTML 文件，可以在该文目录下的 html 文件夹下找到，但在 GitHub 上直接打开 HTML 文件只能看到源代码，看不到显示效果，如果想要看到正常文件内容，可以打开  [htmlpreview](https://htmlpreview.github.io/)，然后复制 HTML 文件的地址链接过去即可看到内容。

---



### 关于反卷积、转置卷积、带孔卷积、Pooling层提高感受野等的理解

- [dilated conv带孔卷积、pooling层提高感受野 反卷积 的理解 - CSDN博客](https://blog.csdn.net/jiachen0212/article/details/78548667)  [荐，看完你会明白的~]

  > pooling 为什么可以提高感受野？得这样理解：首先它第一个作用是降低 feature map 的尺寸，减少需要训练的参数；其次，因为有缩小的作用，所以之前的 4 个像素点，现在压缩成 1 个。那么，相当于我透过这 1 个点，就可以看到前面的 4 个点，这不就是把当前 map 的感受野一下子放大了嘛。所以就有以下结论：pooling 降维减少参数量，并且增大感受野。
  >
  > 。。。
  >
  > 因此，提出了 dilated conv 带孔卷积，解决保持感受野和保护图像尺寸间的互斥问题。
  >
  > 需要好好理解的地方是：带孔卷积并不是卷积核里带孔，而是在卷积的时候，跳着的去卷积 map（比如 dilated＝2 的孔卷积，就是隔一个像素点，“卷”一下，这就相当于把卷积核给放大了（3x3 的核变成 7x7的核，多出位置的 weights 给 0 就是。）这样就使得 3x3 的卷积核也能达到 7x7 卷积核的感受野（因为它在原 map 上，确实 9 个点就跨越覆盖到了传统 conv 的49个像素点），也就扩大了感受野了，使得少一些 pooling 层也没关系。。。

同时推荐看下下面的文章：

- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解 | eamlife's blog](<https://lonepatient.top/2018/02/27/understand-convolution.html>)
- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解 - CSDN](<https://blog.csdn.net/Chaolei3/article/details/79374563>)



## 1. 深度学习之笔记梳理

### (1) 我的理解：神经网络参数改变过程？

见：[我的理解：神经网络参数改变过程？.md](./我的理解：神经网络参数改变过程？.md)

### (2)  什么是端对端训练？

《深度学习之美：AI时代的数据处理与最佳实践》：

> P9：“end to end”（端对对）说的是，输入的是原始数据（始端），然后输出的直接就是最终目标（末端），中间过程不可知。

### (3) 机器学习中训练集、验证集、测试集如何划分

1.传统的机器学习领域中，由于收集到的数据量往往不多，比较小，所以需要将收集到的数据分为三类：训练集、验证集、测试集。也有人分为两类，就是不需要测试集。

比例根据经验不同而不同，这里给出一个例子，如果是三类，可能是训练集：验证集：测试集=6:2:2；如果是两类，可能是训练集：验证集=7:3。因为数据量不多，所以验证集和测试集需要占的数据比例比较多。

2.在大数据时代的机器学习或者深度学习领域中，如果还是按照传统的数据划分方式不是十分合理，因为测试集和验证集用于评估模型和选择模型，所需要的数据量和传统的数据量差不多，但是由于收集到的数据远远大于传统机器学习时代的数据量，所以占的比例也就要缩小。比如我们拥有 1000000，这么多的数据，训练集：验证集：测试集=98:1:1。如果是两类，也就是相同的道理。

注意：有些人在把数据分类的时候是没有测试集数据，这样并不是十分合理，有测试集比较放心，建议把数据分类最好有这个数据集，也就是分为三类数据。



## 2. 深度学习之数学基础

### (1) 什么是标准差和方差？

推荐阅读：[标准差和方差](https://www.shuxuele.com/data/standard-deviation.html)

总结：当数据比较分散时，标准差也比较大，即**数据越分数，标准差也越大**。另外，试着体会这句话：**“差”的意思是离正常有多远。**

### (2) 什么是中心化和标准化？

中心化（又叫零均值化）：是指变量减去它的均值。其实就是一个平移的过程，平移后所有数据的中心是（0，0）。

``` xml
零均值化就是一组数据，其中每一个都减去这组的平均值。
例如，对1、2、3、4、5零均值化，
先算出其均值为3，然后每一个数都减去3，
得到-2、-1、0、1、2
就实现了零均值化
```

标准化（又叫归一化）： 是指数值减去均值，再除以标准差。

参考：

- [中心化（又叫零均值化）和标准化（又叫归一化）](https://blog.csdn.net/GoodShot/article/details/80373372)
- [数据预处理之中心化（零均值化）与标准化（归一化）](https://www.cnblogs.com/wangqiang9/p/9285594.html)

### (3) 什么是正太分布（又名高斯分布）？

正态分布（Normal distribution）又名高斯分布（Gaussian distribution），是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。——from：<https://blog.csdn.net/renwudao24/article/details/44463489>

推荐阅读：[正态分布为什么常见？ - 阮一峰的网络日志](http://www.ruanyifeng.com/blog/2017/08/normal-distribution.html)

统计学里面，正态分布（normal distribution）最常见。男女身高、寿命、血压、考试成绩、测量误差等等，都属于正态分布。

。。。。。。

比如，财富的分布就是不对称的，富人的有钱程度（可能比平均值高出上万倍），远远超出穷人的贫穷程度（平均值的十分之一就是赤贫了），即财富分布曲线有右侧的长尾。相比来说，身高的差异就小得多，最高和最矮的人与平均身高的差距，都在30%多。

这是为什么呢，财富明明也受到多种因素的影响，怎么就不是正态分布呢？

原来，正态分布只适合各种因素累加的情况，如果这些因素不是彼此独立的，会互相加强影响，那么就不是正态分布了。一个人是否能够挣大钱，由多种因素决定：

- 家庭
- 教育
- 运气
- 工作
- ...

这些因素都不是独立的，会彼此加强。如果出生在上层家庭，那么你就有更大的机会接受良好的教育、找到高薪的工作、遇见好机会，反之亦然。也就是说，这不是 1 + 1 = 2 的效果，而是 1 + 1 > 2。

统计学家发现，如果各种因素对结果的影响不是相加，而是相乘，那么最终结果不是正态分布，而是[对数正态分布](https://baike.baidu.com/item/%E5%AF%B9%E6%95%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)（log normal distribution），即`x`的对数值`log(x)`满足正态分布。

这就是说，财富的对数值满足正态分布。如果平均财富是10,000元，那么1000元～10,000元之间的穷人（比平均值低一个数量级，宽度为9000）与10,000元~100,000元之间的富人（比平均值高一个数量级，宽度为90,000）人数一样多。因此，财富曲线左侧的范围比较窄，右侧出现长尾。

推荐阅读：[怎样用通俗易懂的文字解释正态分布及其意义？ - 知乎](<https://www.zhihu.com/question/56891433>)

### (4) 什么是概率分布？

参考：

- [应该如何理解概率分布函数和概率密度函数？ - 简书](<https://www.jianshu.com/p/b570b1ba92bb>)
- [读了本文，你就懂了概率分布 - 知乎](<https://zhuanlan.zhihu.com/p/26810566>)
- [概率分布 - MBA智库百科](<https://wiki.mbalib.com/wiki/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83>)

### (5) 什么是鞍点？

关于「鞍点」的定义：

（1）鞍点附近的某些点比鞍点有更大的代价，而其他点则有更小的代价。

（2）一个不是局部极值点的驻点称为鞍点。

Notes：

- 驻点：一阶导数为0；
- 拐点：二阶导数为0。

Example：

（1）单变量函数：

鞍点处的一阶导为0，二阶导换正负号。 

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190311103632.gif)

（2）多变量函数：

鞍点处，在某些方向上是峰顶，在其他方向上是谷底。 

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190311111410.jpg)

**Note：** 

- 在**高维空间**中，**局部最优**很**罕见**，**鞍点**很**常见** (在低维空间中则相反)；
- 对于模型而言，它**并不知道**自己究竟走到的是驻点还是局部最优点。所幸的是我们常用**带动量的 SGD**。

参考：[深度学习: 鞍点 - Online Note - CSDN博客](https://blog.csdn.net/JNingWei/article/details/79801699)

### (6) 凸优化和非凸优化

为什么要求是凸函数呢？因为如果是下图这样的函数，则无法获得全局最优解。

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190408123740.png)

为什么要求是凸集呢？因为如果可行域不是凸集，也会导致局部最优：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190408123814.png)

之所以要区分凸优化问题和非凸的问题原因在于凸优化问题中局部最优解同时也是全局最优解，这个特性使凸优化问题在一定意义上更易于解决，而一般的非凸最优化问题相比之下更难解决。

非凸优化问题如何转化为凸优化问题的方法：

- 1）修改目标函数，使之转化为凸函数

- 2）抛弃一些约束条件，使新的可行域为凸集并且包含原可行域

参考：[凸优化和非凸优化 - CSDN](<https://blog.csdn.net/kebu12345678/article/details/54926287>)

## 3. 深度学习之CV有关问题

### (1) top5错误率

参考：[什么是图像分类的Top-5错误率？ - 知乎](https://www.zhihu.com/question/36463511)

- 回答1：imagenet图像通常有1000个可能的类别，对每幅图像你可以猜5次结果(即同时预测5个类别标签)，当其中有任何一次预测对了，结果都算对，当5次全都错了的时候，才算预测错误，这时候的分类错误率就叫top5错误率。

- 回答2：翻译一下和Yong Pan答案是一样的，top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。

### (2) 条件随机场CRF在语义分割上的应用

对于每个像素位置 i 具有隐变量 (这里隐变量就是像素的真实类别标签，如果预测结果有 21 类，则 (i∈1,2,..,21)，还有对应的观测值 yi (即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个“条件随机场”（ Conditional Random Field，简称 CRF）。通过观测变量 yi 来推测像素位置 i 对应的类别标签 xi。条件随机场示意图如下：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194042.png)

参考：[语义分割论文-DeepLab系列](http://t.cn/E2z2Bs6)

很多以深度学习为框架的图像语义分割系统都使用了一种叫做 “条件随机场”（ Conditional Random Field，简称 **CRF**）的技术作为输出结果的优化后处理手段。其实类似技术种类较多，比如还有马尔科夫随机场 (MRF) 和高斯条件随机场 (G-CRF) 用的也比较多，但原理都较为类似。

简单来介绍一下 “条件随机场” 的概念。

FCN 是像素到像素的影射，所以最终输出的图片上每一个像素都是标注了分类的，将这些分类简单地看成是不同的变量，每个像素都和其他像素之间建立一种连接，连接就是相互间的关系。于是就会得到一个 “完全图”：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194723.png)



上图是以 4x6 大小的图像像素阵列表示的简易版。那么在全链接的 CRF 模型中，有一个对应的能量函数：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194936.png)

嗯，不要问我这个公式里各种符号是啥，我看不懂。但是我知道这个公式是干嘛滴：

其中等号右边第一个一元项，表示像素对应的语义类别，其类别可以由 FCN 或者其他语义分割模型的预测结果得到；而第二项为二元项，二元项可将像素之间的语义联系 / 关系考虑进去。

这么说太抽象，举个简单的例子，“天空”和 “鸟” 这样的像素在物理空间是相邻的概率，应该要比 “天空” 和 “鱼” 这样像素相邻的概率大，那么天空的边缘就更应该判断为鸟而不是鱼（从概率的角度）。

通过对这个能量函数优化求解，把明显不符合事实识别判断剔除，替换成合理的解释，得到对 FCN 的图像语义预测结果的优化，生成最终的语义分割结果。

*——from：[十分钟看懂图像语义分割技术 | 雷锋网](https://www.leiphone.com/news/201705/YbRHBVIjhqVBP0X5.html)*





---

*update：2019-03-03* 